{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "train_dir = '../train'\n",
    "validation_dir = '../validation'\n",
    "test_dir = '../test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import image_dataset_from_directory\n",
    "\n",
    "IMG_SIZE = 150\n",
    "\n",
    "train_dataset = image_dataset_from_directory(train_dir, label_mode='categorical', image_size=(IMG_SIZE, IMG_SIZE))\n",
    "validation_dataset = image_dataset_from_directory(validation_dir, label_mode='categorical', image_size=(IMG_SIZE, IMG_SIZE))\n",
    "test_dataset = image_dataset_from_directory(test_dir, label_mode='categorical', image_size=(IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "#### Dividir o dataset ####\n",
    "def get_dataset_size(dataset):\n",
    "    return sum(1 for _ in dataset)\n",
    "\n",
    "train_size = get_dataset_size(train_dataset)\n",
    "validation_size = get_dataset_size(validation_dataset)\n",
    "\n",
    "part_train_size = train_size // 6\n",
    "part_validation_size = validation_size // 6\n",
    "\n",
    "def split_dataset(dataset, part_size):\n",
    "    parts = []\n",
    "    for i in range(6):\n",
    "        parts.append(dataset.skip(i * part_size).take(part_size))\n",
    "    return parts\n",
    "\n",
    "train_parts = split_dataset(train_dataset, part_train_size)\n",
    "validation_parts = split_dataset(validation_dataset, part_validation_size)\n",
    "\n",
    "train_dataset_1, train_dataset_2, train_dataset_3, train_dataset_4, train_dataset_5, train_dataset_6 = train_parts\n",
    "validation_dataset_1, validation_dataset_2, validation_dataset_3, validation_dataset_4, validation_dataset_5, validation_dataset_6 = validation_parts\n",
    "\n",
    "print(f\"Train dataset parts sizes: {[get_dataset_size(part) for part in train_parts]}\")\n",
    "print(f\"Validation dataset parts sizes: {[get_dataset_size(part) for part in validation_parts]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "def visualize_images(dataset, num_images=9):\n",
    "    \n",
    "    dataset_iter = iter(dataset)\n",
    "    \n",
    "    images, labels = next(dataset_iter)\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(num_images):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\")) \n",
    "        plt.title(f\"Label: {labels[i].numpy()}\")\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# Exemplo de uso da funcao acima para visualizar uma imagem por cada um dos 6 datasets\n",
    "visualize_images(train_dataset_1, num_images=1)\n",
    "visualize_images(train_dataset_2, num_images=1)\n",
    "visualize_images(train_dataset_3, num_images=1)\n",
    "visualize_images(train_dataset_4, num_images=1)\n",
    "visualize_images(train_dataset_5, num_images=1)\n",
    "visualize_images(train_dataset_6, num_images=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg19 import VGG19\n",
    "\n",
    "conv_base = VGG19(weights=\"imagenet\", include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3)) \n",
    "conv_base.trainable = True\n",
    "\n",
    "# Deixar todas as camadas, exceto as últimas cinco, não treináveis (congeladas)\n",
    "for layer in conv_base.layers[:-5]: \n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Função para imprimir o estado das camadas de forma recursiva\n",
    "def print_layer_trainable_status(layer, indent=0):\n",
    "    print(f\"{' ' * indent}Layer: {layer.name}, Trainable: {layer.trainable}\")\n",
    "    if isinstance(layer, Model):\n",
    "        for sub_layer in layer.layers:\n",
    "            print_layer_trainable_status(sub_layer, indent + 2)\n",
    "\n",
    "print_layer_trainable_status(conv_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "#Adicionar o modelo treinado com feature extraction\n",
    "base_model = keras.models.load_model('models/ModelT_transferLearning_featureExtraction_WithoutDataAumentation_OnlyClassification.h5')\n",
    "\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import GaussianNoise\n",
    "\n",
    "# Definir a sequência de data augmentation\n",
    "data_augmentation = tf.keras.Sequential(\n",
    "    [\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.1),\n",
    "        layers.RandomZoom(0.2),\n",
    "        layers.RandomContrast(0.2),\n",
    "        GaussianNoise(stddev=0.1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Criar uma camada de input com o mesmo shape da VGG19\n",
    "input_layer = Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "\n",
    "# Aplicar data augmentation à camada de input\n",
    "augmented_input = data_augmentation(input_layer)\n",
    "\n",
    "# Passar a camada de data_augmentation como input para o modelo base VGG19\n",
    "vgg19_output = conv_base(augmented_input)\n",
    "\n",
    "# Passar a saída da VGG19 para o modelo pré-treinado\n",
    "model_output = base_model(vgg19_output)\n",
    "\n",
    "# Criar o novo modelo combinado com a VGG19 e o modelo pré-treinado\n",
    "model = Model(inputs=input_layer, outputs=model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_layer_trainable_status(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A partir deste bloco iremos treinar o modelo para os sub datasets\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath='models/ModelT_transferLearning_fineTuning_WithDataAumentation_best.h5',\n",
    "    save_best_only=True,\n",
    "    monitor='val_loss',\n",
    "    verbose=1 \n",
    ")\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer=optimizers.Adam(learning_rate=1e-5, weight_decay=1e-1),metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subset 1\n",
    "history = model.fit(train_dataset_1, epochs=10, validation_data=validation_dataset_1, batch_size=128, callbacks=[checkpoint_callback,early_stopping, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subset 2\n",
    "model.compile(loss='categorical_crossentropy',optimizer=optimizers.Adam(learning_rate=1e-7, weight_decay=1e-1),metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=1e-8)\n",
    "\n",
    "history = model.fit(train_dataset_2, epochs=5, validation_data=validation_dataset_2, batch_size=128, callbacks=[checkpoint_callback,early_stopping, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subset 3\n",
    "model.compile(loss='categorical_crossentropy',optimizer=optimizers.Adam(learning_rate=1e-5, weight_decay=1e-1),metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=1e-7)\n",
    "\n",
    "history = model.fit(train_dataset_3, epochs=5, validation_data=validation_dataset_3, batch_size=128, callbacks=[checkpoint_callback,early_stopping, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in conv_base.layers[-10:]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_layer_trainable_status(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subset 3\n",
    "model.compile(loss='categorical_crossentropy',optimizer=optimizers.Adam(learning_rate=1e-6, weight_decay=1e-2),metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=1e-8)\n",
    "\n",
    "history = model.fit(train_dataset_3, epochs=4, validation_data=validation_dataset_3, batch_size=128, callbacks=[checkpoint_callback,early_stopping, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subset 4\n",
    "model.compile(loss='categorical_crossentropy',optimizer=optimizers.Adam(learning_rate=1e-5, weight_decay=1e-1),metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=1e-8)\n",
    "\n",
    "history = model.fit(train_dataset_4, epochs=5, validation_data=validation_dataset_4, batch_size=128, callbacks=[checkpoint_callback,early_stopping, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subset 5\n",
    "model.compile(loss='categorical_crossentropy',optimizer=optimizers.Adam(learning_rate=1e-6, weight_decay=1e-2),metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=1e-8)\n",
    "\n",
    "history = model.fit(train_dataset_5, epochs=5, validation_data=validation_dataset_5, batch_size=128, callbacks=[checkpoint_callback,early_stopping, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subset 6\n",
    "model.compile(loss='categorical_crossentropy',optimizer=optimizers.Adam(learning_rate=1e-7, weight_decay=1e-2),metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=1e-9)\n",
    "\n",
    "history = model.fit(train_dataset_6, epochs=5, validation_data=validation_dataset_6, batch_size=128, callbacks=[checkpoint_callback,early_stopping, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subset 3\n",
    "model.compile(loss='categorical_crossentropy',optimizer=optimizers.Adam(learning_rate=1e-8, weight_decay=1e-2),metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=1e-10)\n",
    "\n",
    "history = model.fit(train_dataset_3, epochs=5, validation_data=validation_dataset_3, batch_size=128, callbacks=[checkpoint_callback,early_stopping, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subset 1\n",
    "model.compile(loss='categorical_crossentropy',optimizer=optimizers.Adam(learning_rate=1e-9, weight_decay=1e-2),metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=1e-10)\n",
    "\n",
    "history = model.fit(train_dataset_1, epochs=5, validation_data=validation_dataset_1, batch_size=128, callbacks=[checkpoint_callback,early_stopping, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subset 2\n",
    "model.compile(loss='categorical_crossentropy',optimizer=optimizers.Adam(learning_rate=1e-10, weight_decay=1e-2),metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=1e-12)\n",
    "\n",
    "history = model.fit(train_dataset_2, epochs=5, validation_data=validation_dataset_2, batch_size=128, callbacks=[checkpoint_callback,early_stopping, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Gráfico da Accuracya\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, acc, 'bo-', label='Accuracy no Treino')\n",
    "plt.plot(epochs, val_acc, 'b-', label='Accuracy na Validação')\n",
    "plt.title('Accuracy no Treino e Validação')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True) \n",
    "plt.xticks(range(1, len(acc) + 1, 2))  \n",
    "plt.yticks(fontsize=12) \n",
    "plt.tight_layout()\n",
    "\n",
    "# Gráfico da Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, loss, 'bo-', label='Loss no Treino')\n",
    "plt.plot(epochs, val_loss, 'b-', label='Loss na Validação')\n",
    "plt.title('Loss no Treino e Validação')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True) \n",
    "plt.xticks(range(1, len(acc) + 1, 2))\n",
    "plt.yticks(fontsize=12) \n",
    "plt.tight_layout()\n",
    "\n",
    "# Mostrar os gráficos\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in conv_base.layers[-15:]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_layer_trainable_status(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subset 2\n",
    "model.compile(loss='categorical_crossentropy',optimizer=optimizers.Adam(learning_rate=1e-11, weight_decay=1e-2),metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=1e-12)\n",
    "\n",
    "history = model.fit(train_dataset_2, epochs=5, validation_data=validation_dataset_2, batch_size=128, callbacks=[checkpoint_callback,early_stopping, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subset 4\n",
    "model.compile(loss='categorical_crossentropy',optimizer=optimizers.Adam(learning_rate=1e-12, weight_decay=1e-2),metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=1e-14)\n",
    "\n",
    "history = model.fit(train_dataset_4, epochs=3, validation_data=validation_dataset_4, batch_size=128, callbacks=[checkpoint_callback,early_stopping, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Gráfico da Accuracya\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, acc, 'bo-', label='Accuracy no Treino')\n",
    "plt.plot(epochs, val_acc, 'b-', label='Accuracy na Validação')\n",
    "plt.title('Accuracy no Treino e Validação')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True) \n",
    "plt.xticks(range(1, len(acc) + 1, 2))  \n",
    "plt.yticks(fontsize=12) \n",
    "plt.tight_layout()\n",
    "\n",
    "# Gráfico da Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, loss, 'bo-', label='Loss no Treino')\n",
    "plt.plot(epochs, val_loss, 'b-', label='Loss na Validação')\n",
    "plt.title('Loss no Treino e Validação')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True) \n",
    "plt.xticks(range(1, len(acc) + 1, 2))\n",
    "plt.yticks(fontsize=12) \n",
    "plt.tight_layout()\n",
    "\n",
    "# Mostrar os gráficos\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "loaded_model = keras.models.load_model('models/ModelT_transferLearning_fineTuning_WithDataAumentation_best.h5')\n",
    "\n",
    "val_loss, val_acc = loaded_model.evaluate(test_dataset) \n",
    "print('val_acc:', val_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
