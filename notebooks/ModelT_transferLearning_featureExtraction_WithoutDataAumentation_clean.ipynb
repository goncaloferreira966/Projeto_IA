{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "train_dir = '../train'\n",
    "validation_dir = '../validation'\n",
    "test_dir = '../test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import image_dataset_from_directory\n",
    "\n",
    "IMG_SIZE = 150\n",
    "\n",
    "train_dataset = image_dataset_from_directory(train_dir, label_mode='categorical', image_size=(IMG_SIZE, IMG_SIZE))\n",
    "validation_dataset = image_dataset_from_directory(validation_dir, label_mode='categorical', image_size=(IMG_SIZE, IMG_SIZE))\n",
    "test_dataset = image_dataset_from_directory(test_dir, label_mode='categorical', image_size=(IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "#### Dividir o dataset ####\n",
    "def get_dataset_size(dataset):\n",
    "    return sum(1 for _ in dataset)\n",
    "\n",
    "train_size = get_dataset_size(train_dataset)\n",
    "validation_size = get_dataset_size(validation_dataset)\n",
    "\n",
    "part_train_size = train_size // 4\n",
    "part_validation_size = validation_size // 4\n",
    "\n",
    "def split_dataset(dataset, part_size):\n",
    "    parts = []\n",
    "    for i in range(4):\n",
    "        parts.append(dataset.skip(i * part_size).take(part_size))\n",
    "    return parts\n",
    "\n",
    "train_parts = split_dataset(train_dataset, part_train_size)\n",
    "validation_parts = split_dataset(validation_dataset, part_validation_size)\n",
    "\n",
    "train_dataset_1, train_dataset_2, train_dataset_3, train_dataset_4 = train_parts\n",
    "validation_dataset_1, validation_dataset_2, validation_dataset_3, validation_dataset_4 = validation_parts\n",
    "\n",
    "print(f\"Train dataset parts sizes: {[get_dataset_size(part) for part in train_parts]}\")\n",
    "print(f\"Validation dataset parts sizes: {[get_dataset_size(part) for part in validation_parts]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "\n",
    "conv_base = VGG19(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "conv_base.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras \n",
    "import numpy as np\n",
    "\n",
    "def get_features_and_labels(dataset): \n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "\n",
    "    for images, labels in dataset:\n",
    "        preprocessed_images = keras.applications.vgg19.preprocess_input(images) \n",
    "        features = conv_base.predict(preprocessed_images) \n",
    "        all_features.append(features)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    return np.concatenate(all_features), np.concatenate(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividimos o dataset em quatro partes\n",
    "#1/4\n",
    "train_features_1, train_labels_1 = get_features_and_labels(train_dataset_1)\n",
    "val_features_1, val_labels_1 = get_features_and_labels(validation_dataset_1)\n",
    "\n",
    "#2/4\n",
    "train_features_2, train_labels_2 = get_features_and_labels(train_dataset_2)\n",
    "val_features_2, val_labels_2 = get_features_and_labels(validation_dataset_2)\n",
    "\n",
    "#3/4\n",
    "train_features_3, train_labels_3 = get_features_and_labels(train_dataset_3)\n",
    "val_features_3, val_labels_3 = get_features_and_labels(validation_dataset_3)\n",
    "\n",
    "#4/4\n",
    "train_features_4, train_labels_4 = get_features_and_labels(train_dataset_4)\n",
    "val_features_4, val_labels_4 = get_features_and_labels(validation_dataset_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "save_path = 'features/ModelT_transferLearning_featureExtraction_WithoutDataAumentation/'\n",
    "\n",
    "np.save(save_path + 'train_features_dataset1.npy', train_features_1)\n",
    "np.save(save_path + 'train_features_dataset2.npy', train_features_2)\n",
    "np.save(save_path + 'train_features_dataset3.npy', train_features_3)\n",
    "np.save(save_path + 'train_features_dataset4.npy', train_features_4)\n",
    "\n",
    "np.save(save_path + 'val_features_dataset1.npy', val_features_1)\n",
    "np.save(save_path + 'val_features_dataset2.npy', val_features_2)\n",
    "np.save(save_path + 'val_features_dataset3.npy', val_features_3)\n",
    "np.save(save_path + 'val_features_dataset4.npy', val_features_4)\n",
    "\n",
    "np.save(save_path + 'train_features_dataset1.npy', train_labels_1)\n",
    "np.save(save_path + 'train_features_dataset2.npy', train_labels_2)\n",
    "np.save(save_path + 'train_features_dataset3.npy', train_labels_3)\n",
    "np.save(save_path + 'train_features_dataset4.npy', train_labels_4)\n",
    "\n",
    "np.save(save_path + 'train_features_dataset1.npy', val_labels_1)\n",
    "np.save(save_path + 'train_features_dataset2.npy', val_labels_2)\n",
    "np.save(save_path + 'train_features_dataset3.npy', val_labels_3)\n",
    "np.save(save_path + 'train_features_dataset4.npy', val_labels_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "inputs = keras.Input(shape=(4, 4, 512))\n",
    "x = layers.Flatten()(inputs)\n",
    "x = layers.Dense(512, activation='relu')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.7)(x)\n",
    "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A partir deste bloco iremos treinar o modelo para os sub datasets\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer=optimizers.Adam(learning_rate=1e-5, weight_decay=1e-2),metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_features_1, train_labels_1,epochs=120, batch_size=32, validation_data=(val_features_1, val_labels_1),callbacks=[early_stopping,reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc,'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss,'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer=optimizers.Adam(learning_rate=1e-6, weight_decay=1e-1),metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-7)\n",
    "\n",
    "history = model.fit(train_features_2, train_labels_2,epochs=120, batch_size=32, validation_data=(val_features_2, val_labels_2),callbacks=[early_stopping,reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc,'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss,'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer=optimizers.Adam(learning_rate=1e-6, weight_decay=0.5),metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-7)\n",
    "\n",
    "history = model.fit(train_features_3, train_labels_3,epochs=120, batch_size=32, validation_data=(val_features_3, val_labels_3),callbacks=[early_stopping,reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc,'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss,'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer=optimizers.Adam(learning_rate=1e-6, weight_decay=0.5),metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-7)\n",
    "\n",
    "history = model.fit(train_features_4, train_labels_4,epochs=120, batch_size=32, validation_data=(val_features_4, val_labels_4),callbacks=[early_stopping,reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc,'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss,'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "inputs = keras.Input(shape=(150, 150, 3))\n",
    "x = keras.applications.vgg19.preprocess_input(inputs) \n",
    "x = conv_base(x)\n",
    "outputs = model(x)\n",
    "full_model = keras.Model(inputs, outputs)\n",
    "\n",
    "full_model.compile(loss='categorical_crossentropy',optimizer=optimizers.Adam(learning_rate=1e-6, weight_decay=0.5),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model.save('ModelT_transferLearning_featureExtraction_WithoutDataAumentation.h5')\n",
    "model.save('ModelT_transferLearning_featureExtraction_WithoutDataAumentation_OnlyClassification.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "loaded_model = keras.models.load_model('models/ModelT_transferLearning_featureExtraction_WithoutDataAumentation.h5')\n",
    "\n",
    "val_loss, val_acc = loaded_model.evaluate(validation_dataset)\n",
    "print('val_acc:', val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_acc = loaded_model.evaluate(test_dataset) \n",
    "print('val_acc:', val_acc)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
