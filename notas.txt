Otimizadores Usados:
- Gradiante Descendente
- RMSpropn
- Adam (Pelo que pesquisei é a melhor escolha para CNN, é o mais popular atualmente)

Por que Adam?
Adaptação da Taxa de Aprendizado: Adam ajusta a taxa de aprendizado de cada parâmetro individualmente, tornando-o muito eficaz em situações onde a escala dos gradientes varia.
Momento e Aceleração: Combina o momento (como o SGD com momento) e a adaptação de taxa de aprendizado (como o RMSprop), o que pode levar a uma convergência mais rápida.
Robustez: É bastante robusto e tende a funcionar bem com uma ampla gama de arquiteturas e problemas diferentes.



Melhor classificação até agora
Epoch 1/20
625/625 [==============================] - 107s 169ms/step - loss: 4.5289 - accuracy: 0.4020 - val_loss: 1.4359 - val_accuracy: 0.6141
Epoch 2/20
625/625 [==============================] - 105s 168ms/step - loss: 2.4623 - accuracy: 0.5045 - val_loss: 1.0985 - val_accuracy: 0.6285
...
Epoch 19/20
625/625 [==============================] - 1878s 3s/step - loss: 0.8997 - accuracy: 0.7067 - val_loss: 1.0191 - val_accuracy: 0.6814
Epoch 20/20
625/625 [==============================] - 1089s 2s/step - loss: 0.8849 - accuracy: 0.7105 - val_loss: 1.0145 - val_accuracy: 0.6843
